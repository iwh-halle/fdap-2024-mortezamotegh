{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![head.png](https://github.com/iwh-halle/FinancialDataAnalytics/blob/master/figures/head.jpg?raw=1)\n",
    "\n",
    "# Financial Data Analytics in Python\n",
    "\n",
    "**Prof. Dr. Fabian Woebbeking**</br>\n",
    "Assistant Professor of Financial Economics\n",
    "\n",
    "IWH - Leibniz Institute for Economic Research</br>\n",
    "MLU - Martin Luther University Halle-Wittenberg\n",
    "\n",
    "fabian.woebbeking@iwh-halle.de"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Natural Language Processing (NLP)\n",
    "\n",
    "You will need a Git/GitHub repository to submit your course deliverables. Consult [**slides.ipynb**](https://github.com/iwh-halle/FinancialDataAnalytics) for help with the tasks below! If you need further assistance, do not hesitate to open a Q&A at https://github.com/iwh-halle/FinancialDataAnalytics/discussions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Sourcing\n",
    "\n",
    "The first stage involves sourcing and reading the textual content of scientific papers. You find an example pdf file in ``../lit/nonanswers.pdf``. Please [download](https://scholar.google.de/) and analyze at least one additional paper of your choice (make sure to commit the paper to your repository).\n",
    "\n",
    "Use an appropriate PDF reading library or tool to programmatically extract the text. You can find an example below, however, you are free to use any Python library you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Let me get back to you” –\n",
      "A machine learning approach to measuring\n",
      "non-answers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install pdfminer.six if you haven't already\n",
    "# You can install it using conda or pip, see \n",
    "  # https://anaconda.org/conda-forge/pdfminer.six\n",
    "  # https://pypi.org/project/pdfminer.six/\n",
    "\n",
    "# Step 2: Import the required module\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Step 3: Extract text from PDF file\n",
    "extracted_text = extract_text('../lit/nonanswers.pdf')\n",
    "print(extracted_text[0:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Pre-processing\n",
    "\n",
    "Pre-processing is a critical step aimed at cleaning and preparing the text data for analysis. Steps that you should consider:\n",
    "\n",
    "* Removing punctuation, numbers and special characters using regular expressions.\n",
    "* Converting all the text to a uniform case (usually lower case) to ensure that the analysis is not case-sensitive.\n",
    "* Stop word removal, i.e. eliminating commonly used words (e.g., 'and', 'the', 'is') that do not contribute significantly to the overall meaning and can skew the analysis.\n",
    "* Other potential pre-processing steps might include stemming and lemmatization, depending on the specific requirements and goals of the analysis. (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Analysis\n",
    "\n",
    "The final stage is the analysis of the pre-processed text, in order to extract meaningful context. This may involve:\n",
    "\n",
    "* Frequency Analysis: Determining the most commonly occurring words or phrases, which can provide initial insights into the primary focus areas of the papers. Consider, e.g. a word cloud as a visualization.\n",
    "* Contextual Analysis: Using more advanced NLP techniques such as Word Embedding or Topic Modeling to understand the context of the papers.\n",
    "* Sentiment analysis: We would expect that scientific papers are written in a neutral tone, can you confirm this?\n",
    "* Summarization: Employing algorithms to generate concise summaries of the papers, capturing the key points and findings.\n",
    "\n",
    "Pick any method that you like (you are allowed to use ChatGPT's API as well)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
